## Sequence models

Early neural network models primarily used hand-engineered sequence features as input to a fully connected neural network [@doi:10.1093/nar/gku1058; @doi:10.1126/science.1254806] (Figure {@fig:sequence_features}).
As convolutional neural network (CNN) approaches matured for image processing and computer vision, researchers leveraged biological sequence proximity similarly.
CNNs are a neural network variant that groups input data by spatial context to extract features for prediction.

The definition of "spatial context" is specific to the input: one might group image pixels that are nearby in 2D space, or genomic base pairs that are nearby in the linear genome.
In this way, CNNs consider context without making strong assumptions about exactly how much context is needed or how it should be encoded; the data informs the encoding.
A detailed description of how CNNs are applied to sequences can be found in Angermueller et al. [@doi:10.15252/msb.20156651].

![Contrasting approaches to extracting features from DNA or RNA sequence data. Early models defined features of interest by hand based on prior knowledge about the prediction or clustering problem of interest, such as GC content or sequence melting point. Convolutional models use sequence convolutions to derive features directly from sequence proximity, without requiring features of interest to be identified before the model is trained.](images/sequence_features.svg){#fig:sequence_features .white}

### Applications in regulatory biology

Many early applications of deep learning to biological sequences were in regulatory biology.
Early CNNs for sequence data predicted binding protein sequence specificity from DNA or RNA sequence [@doi:10.1038/nbt.3300], variant effects from noncoding DNA sequence [@doi:10.1038/nmeth.3547], and chromatin accessibility from DNA sequence [@doi:10.1101/gr.200535.115].

Recent sequence models take advantage of hardware advances and methodological innovation to incorporate more sequence context and rely on fewer modeling assumptions.
BPNet, a CNN that predicts transcription factor binding profiles from DNA sequences, accurately mapped known locations of binding motifs in mouse embryonic stem cells [@doi:10.1101/737981].
BPNet considers 1000 base pairs of context around each position when predicting binding probabilities with a technique called dilated convolutions [@arxiv:1511.07122], which is particularly important because motif spacing and periodicity can influence binding.
cDeepbind [@doi:10.1101/345140] combines RNA sequences with information about secondary structure to predict RNA binding protein affinities.
Its convolutional model acts on a feature vector combining sequence and structural information, using context for both to inform predictions.
APARENT [@doi:10.1016/j.cell.2019.04.046] is a CNN that predicts alternative polyadenylation (APA) from a training set of over 3 million synthetic APA reporter sequences.
These diverse applications underscore the power of modern deep learning models to synthesize large sequence datasets.

Models that consider sequence context have also been applied to epigenetic data.
DeepSignal [@doi:10.1093/bioinformatics/btz276] is a CNN that uses contextual electrical signals from Oxford Nanopore single-molecule sequencing data to predict 5mC or 6mA DNA methylation status.
MRCNN [@doi:10.1186/s12864-019-5488-5] uses sequences of length 400, centered at CpG sites, to predict 5mC methylation status.
Deep learning models have also been used to predict gene expression from histone modifications [@doi:10.1101/329334; @doi:10.1093/bioinformatics/bty612].
Here, a neural network model consisting of long short-term memory (LSTM) units was used to encode the long-distance interactions of histone marks in both the 3' and 5' genomic directions.
In each of these cases, proximity in the linear genome helped model the complex interactions between DNA sequence and epigenome.

### Applications in variant calling and mutation detection

Identification of genetic variants also benefits from models that include sequence context.
DeepVariant [@doi:10.1038/nbt.4235] applies a CNN to images of sequence read pileups, using read data around each candidate variant to accurately distinguish true variants from sequencing errors.
<!-- could mention GATK4 here which uses a CNN, although nothing has been published? -->
<!-- https://gatkforums.broadinstitute.org/gatk/discussion/10996/deep-learning-in-gatk4 -->
CNNs have also been applied to single molecule (PacBio and Oxford Nanopore) sequencing data [@doi:10.1038/s41467-019-09025-z], using a different sequence encoding that results in better performance than DeepVariant on single molecule data.
However, many variant calling models still use hand-engineered sequence features as input to a classifier, including current state-of-the-art approaches to insertion/deletion calling [@doi:10.1101/601450; @doi:10.1101/628222].
Detection of somatic mutations is a distinct but related challenge to detection of germline variants, and has also recently benefitted from use of CNNs [@doi:10.1038/s41467-019-09027-x].

